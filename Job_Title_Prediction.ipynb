{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Job_Title_Prediction.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzTSaXMtCsd/3fMn9CzIAD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajeshkpandey/AWESOME-FER/blob/master/Job_Title_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\"./\"))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import randint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import StringIO\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "KnESDf11k2ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df.head()\n",
        "test_df = pd.read_csv('test.csv')\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "k4csU4nPW68-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n",
        "test_df.shape"
      ],
      "metadata": {
        "id": "IRrQ9qvJ_-m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()\n",
        "test_df.tail()"
      ],
      "metadata": {
        "id": "3bwxOlQoXa6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "#cleaning training dataset\n",
        "for i in range(len(df)):\n",
        "  df['Job Description'][i] = df['Job Description'][i].lower()\n",
        "  df['Job Description'][i]=\"\".join(char for char in df['Job Description'][i] if char not in string.punctuation)\n",
        "  ps = PorterStemmer()\n",
        "  wml = WordNetLemmatizer()\n",
        "  df['Job Description'][i]=ps.stem(df['Job Description'][i])\n",
        "  df['Job Description'][i]=wml.lemmatize(df['Job Description'][i])\n",
        "  df['Job Description'][i]=df['Job Description'][i].split()\n",
        "  filter_words = []\n",
        "  Stopwords = set(stopwords.words('english'))\n",
        "  for word in df['Job Description'][i]:\n",
        "    if word not in Stopwords:\n",
        "         filter_words.append(word)\n",
        "  df['Job Description'][i] = filter_words \n",
        "  df['Job Description'][i] = \" \".join(df['Job Description'][i])\n",
        "\n",
        "#cleaning testing set\n",
        "for i in range(len(test_df)):\n",
        "  test_df['Job Description'][i] = test_df['Job Description'][i].lower()\n",
        "  test_df['Job Description'][i]=\"\".join(char for char in test_df['Job Description'][i] if char not in string.punctuation)\n",
        "  ps = PorterStemmer()\n",
        "  wml = WordNetLemmatizer()\n",
        "  test_df['Job Description'][i]=ps.stem(test_df['Job Description'][i])\n",
        "  test_df['Job Description'][i]=wml.lemmatize(test_df['Job Description'][i])\n",
        "  test_df['Job Description'][i]=test_df['Job Description'][i].split()\n",
        "  filter_words = []\n",
        "  Stopwords = set(stopwords.words('english'))\n",
        "  for word in test_df['Job Description'][i]:\n",
        "    if word not in Stopwords:\n",
        "         filter_words.append(word)\n",
        "  test_df['Job Description'][i] = filter_words\n",
        "  test_df['Job Description'][i] = \" \".join(test_df['Job Description'][i])\n",
        " "
      ],
      "metadata": {
        "id": "WxQP_CWRAJOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new dataframe with two columns\n",
        "df1 = df[['Job Description', 'Title']].copy()\n",
        "\n",
        "# Remove missing values (NaN)\n",
        "df1 = df1[pd.notnull(df1['Job Description'])]\n",
        "\n",
        "# Renaming job description column for a simpler name\n",
        "df1.columns = ['Job_Description', 'Title'] \n",
        "\n",
        "df1.shape"
      ],
      "metadata": {
        "id": "RrGCaB5UlpZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.tail()"
      ],
      "metadata": {
        "id": "yA6mQ0UHc2fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of Job Description with text\n",
        "total = df1['Job_Description'].notnull().sum()\n",
        "round((total/len(df)*100),1)"
      ],
      "metadata": {
        "id": "_r6i_E5qmIbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(df.Title.unique()).values"
      ],
      "metadata": {
        "id": "DoBCCKFkmlcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.Title.unique())"
      ],
      "metadata": {
        "id": "XxfYWU4anBEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df1.sample(20000, random_state=1).copy()"
      ],
      "metadata": {
        "id": "UVjXMM2enVV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(df2.Title.unique())"
      ],
      "metadata": {
        "id": "JEJeE_FKibfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'category_id' with encoded categories \n",
        "df2['category_id'] = df2['Title'].factorize()[0]\n",
        "category_id_df = df2[['Title', 'category_id']].drop_duplicates()\n",
        "\n",
        "\n",
        "# Dictionaries for future use\n",
        "category_to_id = dict(category_id_df.values)\n",
        "id_to_category = dict(category_id_df[['category_id', 'Title']].values)\n",
        "\n",
        "# New dataframe\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "8OHSUuSNnYli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.tail()"
      ],
      "metadata": {
        "id": "B0M9BWA3nzOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
        "                        ngram_range=(1, 2), \n",
        "                        stop_words='english')\n",
        "\n",
        "# We transform each Job Description into a vector\n",
        "features = tfidf.fit_transform(df2.Job_Description).toarray()\n",
        "\n",
        "labels = df2.category_id\n",
        "\n",
        "print(\"Each of the %d job descriptions is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(features.shape))"
      ],
      "metadata": {
        "id": "RD0oQRoHqeb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df2['Job_Description'] # Collection of Job descriptions\n",
        "y = df2['Title'] # Target or the labels we want to predict\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state = 0)"
      ],
      "metadata": {
        "id": "0cikKjotsrqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\n",
        "    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n",
        "    LinearSVC(),\n",
        "    MultinomialNB(),\n",
        "    LogisticRegression(random_state=0),\n",
        "]\n",
        "\n",
        "# 5 Cross-validation\n",
        "CV = 5\n",
        "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
        "\n",
        "entries = []\n",
        "for model in models:\n",
        "  model_name = model.__class__.__name__\n",
        "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
        "  for fold_idx, accuracy in enumerate(accuracies):\n",
        "    entries.append((model_name, fold_idx, accuracy))\n",
        "    \n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
      ],
      "metadata": {
        "id": "SZsZxWg4teXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
        "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
        "\n",
        "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
        "          ignore_index=True)\n",
        "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
        "acc"
      ],
      "metadata": {
        "id": "QoiD_68fttaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test,indices_train,indices_test = train_test_split(features, \n",
        "                                                               labels, \n",
        "                                                               df2.index, test_size=0.25, \n",
        "                                                               random_state=1)\n",
        "model = LinearSVC()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "OayttVcqt773"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print('\\t\\t\\t\\tCLASSIFICATIION METRICS\\n')\n",
        "print(metrics.classification_report(y_test, y_pred, \n",
        "                                    target_names= df2['Product'].unique()))"
      ],
      "metadata": {
        "id": "DTwGIiM4uEin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
        "                                                    test_size=0.25,\n",
        "                                                    random_state = 0)\n",
        "\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
        "                        ngram_range=(1, 2), \n",
        "                        stop_words='english')\n",
        "\n",
        "fitted_vectorizer = tfidf.fit(X_train)\n",
        "tfidf_vectorizer_vectors = fitted_vectorizer.transform(X_train)\n",
        "\n",
        "model = LinearSVC().fit(tfidf_vectorizer_vectors, y_train)"
      ],
      "metadata": {
        "id": "vWdT67sWusXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=[]\n",
        "for i in range(len(test_df)):\n",
        "  predictions.append(model.predict(fitted_vectorizer.transform([test_df['Job Description'][i]])))"
      ],
      "metadata": {
        "id": "I4FWTAiRvPme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)"
      ],
      "metadata": {
        "id": "CsDpmAuqv-yS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}